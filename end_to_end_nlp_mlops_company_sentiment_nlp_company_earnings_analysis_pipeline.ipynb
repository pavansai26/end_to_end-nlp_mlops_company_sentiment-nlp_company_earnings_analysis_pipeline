{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/QCIM3anFXvkC2hG2zKwC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/end_to_end-nlp_mlops_company_sentiment-nlp_company_earnings_analysis_pipeline/blob/main/end_to_end_nlp_mlops_company_sentiment_nlp_company_earnings_analysis_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Understanding Trends in Company Valuation with NLP**"
      ],
      "metadata": {
        "id": "6jL3ppilUvGX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Orchestrating company earnings trend analysis, using SEC filings, news sentiment with the transformers"
      ],
      "metadata": {
        "id": "nwgft4IuVJgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we demonstrate how to summarize and derive sentiments out of Security and Exchange Commission reports filed by a publicly traded organization. We will derive the overall market sentiments about the said organization through financial news articles within the same financial period to present a fair view of the organization vs. market sentiments and outlook about the company's overall valuation and performance. In addition to this we will also identify the most popular keywords and entities within the news articles about that organization.\n",
        "\n",
        "In order to achieve the above we will be using multiple SageMaker Hugging Face based NLP transformers for the downstream NLP tasks of Summarization (e.g., of the news and SEC MDNA sections) and Sentiment Analysis (of the resulting summaries)."
      ],
      "metadata": {
        "id": "hqxT136dVQJn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HZs53yOSUt0y"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Amazon SageMaker Pipelines is the first purpose-built, easy-to-use continuous integration and continuous delivery (CI/CD) service for machine learning (ML). With SageMaker Pipelines, you can create, automate, and manage end-to-end ML workflows at scale.\n",
        "\n",
        "Orchestrating workflows across each step of the machine learning process (e.g. exploring and preparing data, experimenting with different algorithms and parameters, training and tuning models, and deploying models to production) can take months of coding.\n",
        "\n",
        "Since it is purpose-built for machine learning, SageMaker Pipelines helps you automate different steps of the ML workflow, including data loading, data transformation, training and tuning, and deployment. With SageMaker Pipelines, you can build dozens of ML models a week, manage massive volumes of data, thousands of training experiments, and hundreds of different model versions. You can share and re-use workflows to recreate or optimize models"
      ],
      "metadata": {
        "id": "_Q6N39NRVqIt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LwfMq1M1Vqwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to demonstrate how to summarize and derive sentiments out of Security and Exchange Commission reports filed by a publicly traded organization. We are also going to derive the overall market sentiments about the said organization through financial news articles within the same financial period to present a fair view of the organization vs. market sentiments and outlook about the company's overall valuation and performance. In addition to this we will also identify the most popular keywords and entities within the news articles about that organization."
      ],
      "metadata": {
        "id": "T5o5Rt4xVwtR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pwn2ZTWwVxOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **install packages**"
      ],
      "metadata": {
        "id": "9scM0X2CZ6D9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install sagemaker-studio-image-build CLI tool\n",
        "!pip install sagemaker-studio-image-build"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JsP_67TWVR8",
        "outputId": "11aa9153-e135-497d-b92c-632778ffd637"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sagemaker-studio-image-build\n",
            "  Downloading sagemaker_studio_image_build-0.6.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto3<2.0,>=1.10.44\n",
            "  Downloading boto3-1.26.129-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.6/135.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sagemaker<3.0\n",
            "  Downloading sagemaker-2.152.0.tar.gz (751 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m751.1/751.1 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.30.0,>=1.29.129\n",
            "  Downloading botocore-1.29.129-py3-none-any.whl (10.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.1-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting attrs<23,>=20.3.0\n",
            "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (2.2.1)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4.0,>=3.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (3.20.3)\n",
            "Collecting protobuf3-to-dict<1.0,>=0.1.5\n",
            "  Downloading protobuf3-to-dict-0.1.5.tar.gz (3.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting smdebug_rulesconfig==1.0.1\n",
            "  Downloading smdebug_rulesconfig-1.0.1-py2.py3-none-any.whl (20 kB)\n",
            "Collecting importlib-metadata<5.0,>=1.4.0\n",
            "  Downloading importlib_metadata-4.13.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (23.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (1.5.3)\n",
            "Collecting pathos\n",
            "  Downloading pathos-0.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting schema\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Collecting PyYAML==5.4.1\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (4.3.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (3.3.0)\n",
            "Requirement already satisfied: tblib==1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker<3.0->sagemaker-studio-image-build) (1.7.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.129->boto3<2.0,>=1.10.44->sagemaker-studio-image-build) (1.26.15)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.129->boto3<2.0,>=1.10.44->sagemaker-studio-image-build) (2.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker<3.0->sagemaker-studio-image-build) (3.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker<3.0->sagemaker-studio-image-build) (1.16.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker<3.0->sagemaker-studio-image-build) (0.19.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker<3.0->sagemaker-studio-image-build) (2022.7.1)\n",
            "Collecting ppft>=1.7.6.6\n",
            "  Downloading ppft-1.7.6.6-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pox>=0.3.2\n",
            "  Downloading pox-0.3.2-py3-none-any.whl (29 kB)\n",
            "Collecting multiprocess>=0.70.14\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill>=0.3.6\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema->sagemaker<3.0->sagemaker-studio-image-build) (0.6.0.post1)\n",
            "Building wheels for collected packages: sagemaker-studio-image-build, sagemaker, PyYAML, protobuf3-to-dict\n",
            "  Building wheel for sagemaker-studio-image-build (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sagemaker-studio-image-build: filename=sagemaker_studio_image_build-0.6.0-py3-none-any.whl size=13466 sha256=9d5ebe6ca5afeff08e0cfb936bd2a82dc638acffc74daa872e20a09de88e0848\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/7b/d1/1318b1530ee5322c9be00f206badea11b5148626ef58c0e0dc\n",
            "  Building wheel for sagemaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sagemaker: filename=sagemaker-2.152.0-py2.py3-none-any.whl size=1007508 sha256=e13773cbc96e212322bbba73aa255aaa72c1c1819c86433fd14a6d0ad122c004\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/20/40/2bb58641443472a4ba7e4ae57279345ab3646253379f7ce878\n",
            "  Building wheel for PyYAML (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=3a5b1aba3965e8f531a730c230ab5382b15e12de0c186e95f05cc2f265d156cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
            "  Building wheel for protobuf3-to-dict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protobuf3-to-dict: filename=protobuf3_to_dict-0.1.5-py3-none-any.whl size=4028 sha256=be4e3aca4d0b2dff931447a40b49fef463cdab57d0ea43ed28ea679fb7e78b78\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/3e/42/e783cdd4e7b8fda9bfc472eeb465bc9041bda90a3dbece8d74\n",
            "Successfully built sagemaker-studio-image-build sagemaker PyYAML protobuf3-to-dict\n",
            "Installing collected packages: smdebug_rulesconfig, schema, PyYAML, protobuf3-to-dict, ppft, pox, jmespath, importlib-metadata, dill, attrs, multiprocess, botocore, s3transfer, pathos, boto3, sagemaker, sagemaker-studio-image-build\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0\n",
            "    Uninstalling PyYAML-6.0:\n",
            "      Successfully uninstalled PyYAML-6.0\n",
            "  Attempting uninstall: attrs\n",
            "    Found existing installation: attrs 23.1.0\n",
            "    Uninstalling attrs-23.1.0:\n",
            "      Successfully uninstalled attrs-23.1.0\n",
            "Successfully installed PyYAML-5.4.1 attrs-22.2.0 boto3-1.26.129 botocore-1.29.129 dill-0.3.6 importlib-metadata-4.13.0 jmespath-1.0.1 multiprocess-0.70.14 pathos-0.3.0 pox-0.3.2 ppft-1.7.6.6 protobuf3-to-dict-0.1.5 s3transfer-0.6.1 sagemaker-2.152.0 sagemaker-studio-image-build-0.6.0 schema-0.7.5 smdebug_rulesconfig-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sagemaker==2.91.1\n",
        "\n",
        "!pip install transformers\n",
        "!pip install typing\n",
        "!pip install sentencepiece\n",
        "!pip install fiscalyear\n",
        "\n",
        "# Install SageMaker Jumpstart Industry\n",
        "!pip install smjsindustry"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XX0os2UNZvp-",
        "outputId": "9c938982-dd18-4079-a6e6-6ed53882d2db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m534.7/534.7 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sagemaker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26321 sha256=a28e0ef9a83b9904056f2159b3bbc676167ef3492839ec0b1d769977e0a07e5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "Successfully built typing\n",
            "Installing collected packages: typing\n",
            "Successfully installed typing-3.7.4.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting fiscalyear\n",
            "  Downloading fiscalyear-0.4.0-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: fiscalyear\n",
            "Successfully installed fiscalyear-0.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting smjsindustry\n",
            "  Downloading smjsindustry-1.0.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from smjsindustry) (23.1)\n",
            "Collecting sagemaker>=2.111.0\n",
            "  Using cached sagemaker-2.152.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (4.3.3)\n",
            "Requirement already satisfied: tblib==1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.7.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.5.3)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.3.0)\n",
            "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.1.5)\n",
            "Requirement already satisfied: schema in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.7.5)\n",
            "Requirement already satisfied: attrs<23,>=20.3.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (20.3.0)\n",
            "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (4.13.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (3.3.0)\n",
            "Requirement already satisfied: PyYAML==5.4.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (5.4.1)\n",
            "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.0.1)\n",
            "Requirement already satisfied: boto3<2.0,>=1.26.28 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.26.129)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.2.0)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (2.2.1)\n",
            "Requirement already satisfied: protobuf<4.0,>=3.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (3.20.3)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.129 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (1.29.129)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (0.6.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.111.0->smjsindustry) (3.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.111.0->smjsindustry) (1.16.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker>=2.111.0->smjsindustry) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker>=2.111.0->smjsindustry) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker>=2.111.0->smjsindustry) (2022.7.1)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (0.3.6)\n",
            "Requirement already satisfied: pox>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (0.3.2)\n",
            "Requirement already satisfied: ppft>=1.7.6.6 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (1.7.6.6)\n",
            "Requirement already satisfied: multiprocess>=0.70.14 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (0.70.14)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema->sagemaker>=2.111.0->smjsindustry) (0.6.0.post1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.129->boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (1.26.15)\n",
            "Installing collected packages: sagemaker, smjsindustry\n",
            "  Attempting uninstall: sagemaker\n",
            "    Found existing installation: sagemaker 2.91.1\n",
            "    Uninstalling sagemaker-2.91.1:\n",
            "      Successfully uninstalled sagemaker-2.91.1\n",
            "Successfully installed sagemaker-2.152.0 smjsindustry-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import packages"
      ],
      "metadata": {
        "id": "a3nMSHibaIZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import botocore\n",
        "import pandas as pd\n",
        "import sagemaker\n",
        "\n",
        "print(f\"SageMaker version: {sagemaker.__version__}\")\n",
        "\n",
        "from sagemaker.huggingface import HuggingFace\n",
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "from sagemaker.workflow.pipeline import Pipeline\n",
        "from sagemaker.workflow.steps import CreateModelStep\n",
        "from sagemaker.workflow.step_collections import RegisterModel\n",
        "from sagemaker.workflow.steps import ProcessingStep\n",
        "from sagemaker.workflow.steps import TransformStep\n",
        "from sagemaker.workflow.properties import PropertyFile\n",
        "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
        "from sagemaker.sklearn.processing import ScriptProcessor\n",
        "from sagemaker.lambda_helper import Lambda\n",
        "from sagemaker.workflow.lambda_step import (\n",
        "    LambdaStep,\n",
        "    LambdaOutput,\n",
        "    LambdaOutputTypeEnum,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxK1uEMrZ16U",
        "outputId": "3dc3085c-5c33-499e-ac49-d34e4ff2c9a3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SageMaker version: 2.152.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Custom Container\n"
      ],
      "metadata": {
        "id": "Y6G3qXQ-V_n7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To achieve that, you first have to build a docker image and push it to an ECR (Elastic Container Registry) repo in your account. Typically, this can be done using the docker CLI and aws cli in your local machine pretty easily. However, SageMaker makes it even easier to use this in the studio environment to build, create, and push any custom container to your ECR repository using a purpose-built tool known as sagemaker-studio-image-build, and use the custom container image in your notebooks for your ML projects."
      ],
      "metadata": {
        "id": "G2uRb9WjWFBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "region = boto3.session().region_name\n",
        "my_account = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
        "nlp_script_processor = f\"nlp-script-processor:1.0\"\n",
        "\n",
        "CONTAINER_IMAGE_URI = f\"{my_account}.dkr.ecr.{region}.amazonaws.com/{nlp_script_processor}\"\n",
        "\n",
        "CONTAINER_IMAGE_URI\n"
      ],
      "metadata": {
        "id": "rwuk0fpjWBmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Grant appropriate permissions to SageMaker"
      ],
      "metadata": {
        "id": "fjySUK6PYPxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to use sagemaker-studio-image-build, we need to first add permissions to SageMaker's IAM role so that it may perform actions on your behalf. Specifically, you would add Amazon ECR and Amazon CodeBuild permissions to it."
      ],
      "metadata": {
        "id": "H8m1m7ZOYVYF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yH-MmNKZYQrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to this, you will also have to add the iam:PassRole permission to the SageMaker Studio execution role. Add the following policy as an inline policy to the SageMaker Studio Execution role using the AWS IAM console."
      ],
      "metadata": {
        "id": "68nJdxs3YfjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F15gesWGYgNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, you must add a trust relationship in the SageMaker Studio Execution role to allow CodeBuild to assume this role"
      ],
      "metadata": {
        "id": "JaLjrXvoYxIE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Action\": \"sts:AssumeRole\""
      ],
      "metadata": {
        "id": "-chVquFqY1IE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBDFFAaBYxpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Build a custom Docker image**"
      ],
      "metadata": {
        "id": "cxfJsa8tY8FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now build a custom Dockerfile and use the CLI tool to build the image from the Dockerfile. Our docker image is going to be pretty simple, it will be a copy of the open source python:3.7-slim-buster image and contain an installation of Boto3 SDK, SageMaker SDK, Pandas, and NumPy.\n",
        "\n",
        "For our NLP pipeline, we have a number of tasks that depend on Boto3 and SageMaker SDK. We will also use the SageMaker JumpStart Industry Python SDK to download 10k/10Q reports from SEC's EDGAR system. We install all of these dependencies in the container, and use the custom container in our ScriptProcessor step in our pipelines."
      ],
      "metadata": {
        "id": "kqAqsfk8ZCkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Dockerfile\n",
        "FROM python:3.7-slim-buster\n",
        "\n",
        "RUN pip3 install smjsindustry==1.0.0 requests botocore boto3>=1.15.0 sagemaker pandas numpy transformers typing sentencepiece nltk\n",
        "RUN python3 -c \"import nltk; nltk.download('punkt')\"\n",
        "ENV PYTHONUNBUFFERED=TRUE\n",
        "\n",
        "ENTRYPOINT [\"python3\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eob4uRfzY-Xn",
        "outputId": "a702aed3-91d0-492f-ff93-b7bfb04e350d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Dockerfile\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code cell above will create a Dockerfile in the local project's directory. We can then run the sm-docker build command to build, and publish our image. This single command will take care of building the Docker image and publishing it to a private ECR Repository in your current region (i.e. your SageMaker Studio's default Region)."
      ],
      "metadata": {
        "id": "i3wQZ5xYZSU8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ccPwkq2UZMki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: You must execute the code cell above to run the following cells. the sm-docker build command reads the Dockerfile to create the docker image. To ensure that the code above ran successfully, please verify that you have a file named Dockerfile is under the same directory where this notebook is located in the left navigation pane of Studio. This project already includes the Dockerfile, however, if you modify the code cell above, it would be a good idea to verify if the contents of the Dockerfile were updated correctly."
      ],
      "metadata": {
        "id": "WMx7HbpBZVSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "!sm-docker build . --repository $nlp_script_processor"
      ],
      "metadata": {
        "id": "yloQTqH3ZfrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SD6CYOoYZqqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define parameters that you'll use throughout the notebook\n"
      ],
      "metadata": {
        "id": "CDbjO2HIad3g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3 = boto3.resource(\"s3\")\n",
        "region = boto3.Session().region_name\n",
        "sagemaker_session = sagemaker.Session()\n",
        "role = sagemaker.get_execution_role()\n",
        "sagemaker_role = role\n",
        "default_bucket = sagemaker_session.default_bucket()\n",
        "prefix = \"nlp-e2e-mlops\"\n",
        "s3_client = boto3.client(\"s3\", region_name=region)\n",
        "sagemaker_boto_client = boto3.client(\"sagemaker\", region_name=region)\n",
        "\n",
        "\n",
        "# deploy_model_instance_type = \"ml.m4.8xlarge\"\n",
        "deploy_model_instance_type = \"ml.m4.xlarge\"\n",
        "inference_instances = [\n",
        "    \"ml.t2.medium\",\n",
        "    \"ml.m5.xlarge\",\n",
        "    \"ml.m5.2xlarge\",\n",
        "    \"ml.m5.4xlarge\",\n",
        "    \"ml.m5.12xlarge\",\n",
        "]\n",
        "transform_instances = [\"ml.m5.xlarge\"]\n",
        "PROCESSING_INSTANCE = \"ml.m4.4xlarge\"\n",
        "ticker = \"AMZN\""
      ],
      "metadata": {
        "id": "796MKUH5ah_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"s3://{default_bucket}/{prefix}/code/model_deploy.py\")\n",
        "print(f\"SageMaker Role: {role}\")"
      ],
      "metadata": {
        "id": "eM52kGXbaop8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define parameters to parametrize Pipeline Execution\n"
      ],
      "metadata": {
        "id": "IVxqI_zxawPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using SageMaker Pipelines, we can define the steps to be included in a pipeline but then use parameters to modify that pipeline when we go to execute the pipeline, without having to modify the pipeline definition. We'll provide some default parameter values that can be overridden on pipeline execution.\n",
        "\n"
      ],
      "metadata": {
        "id": "m7a6peZoa11d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define some default parameters:\n",
        "\n",
        "# specify default number of instances for processing step\n",
        "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
        "\n",
        "# specify default instance type for processing step\n",
        "processing_instance_type = ParameterString(\n",
        "    name=\"ProcessingInstanceType\", default_value=PROCESSING_INSTANCE\n",
        ")\n",
        "\n",
        "# specify location of inference data for data processing step\n",
        "inference_input_data = f\"s3://{default_bucket}/{prefix}/nlp-pipeline/inf-data\"\n",
        "\n",
        "# Specify the Ticker CIK for the pipeline\n",
        "inference_ticker_cik = ParameterString(\n",
        "    name=\"InferenceTickerCik\",\n",
        "    default_value=ticker,\n",
        ")\n",
        "\n",
        "# specify default method for model approval\n",
        "model_approval_status = ParameterString(\n",
        "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
        ")\n",
        "\n",
        "# specify if new model needs to be registered and deployed\n",
        "model_register_deploy = ParameterString(name=\"ModelRegisterDeploy\", default_value=\"Y\")"
      ],
      "metadata": {
        "id": "MhI0avABazQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing SEC dataset\n"
      ],
      "metadata": {
        "id": "E25zQH4Jbptm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we dive right into setting up the pipeline, let's take a look at how the SageMaker Jumpstart Industry SDK for Financial language model helps obtain the dataset from SEC forms and what are the features available for us to use.\n",
        "\n"
      ],
      "metadata": {
        "id": "OH4ZF-K5btex"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XtKxiwyxbrdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install the SageMaker JumpStart Industry SDK\n"
      ],
      "metadata": {
        "id": "N41rK3nbbwkK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functionality is delivered through a client-side SDK. The first step requires pip installing a Python package that interacts with a SageMaker processing container. The retrieval, parsing, transforming, and scoring of text is a complex process and uses different algorithms and packages. In order to make this seamless and stable for the user, the functionality is packaged into a SageMaker container. This lifts the load of installation and maintenance of the workflow, reducing the user effort down to a pip install followed by a single API call."
      ],
      "metadata": {
        "id": "w3VQEY8Jb2PF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-index smjsindustry\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAQ-iLgvbw8N",
        "outputId": "24584e69-2697-46b4-c272-0021478ba9a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: smjsindustry in /usr/local/lib/python3.10/dist-packages (1.0.2)\n",
            "Requirement already satisfied: sagemaker>=2.111.0 in /usr/local/lib/python3.10/dist-packages (from smjsindustry) (2.152.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from smjsindustry) (23.1)\n",
            "Requirement already satisfied: schema in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.7.5)\n",
            "Requirement already satisfied: attrs<23,>=20.3.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (20.3.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (4.3.3)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (3.3.0)\n",
            "Requirement already satisfied: google-pasta in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.2.0)\n",
            "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.0.1)\n",
            "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.1.5)\n",
            "Requirement already satisfied: tblib==1.7.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.7.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.5.3)\n",
            "Requirement already satisfied: PyYAML==5.4.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (5.4.1)\n",
            "Requirement already satisfied: protobuf<4.0,>=3.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (3.20.3)\n",
            "Requirement already satisfied: pathos in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (0.3.0)\n",
            "Requirement already satisfied: boto3<2.0,>=1.26.28 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.26.129)\n",
            "Requirement already satisfied: cloudpickle==2.2.1 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (2.2.1)\n",
            "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (4.13.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from sagemaker>=2.111.0->smjsindustry) (1.22.4)\n",
            "Requirement already satisfied: botocore<1.30.0,>=1.29.129 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (1.29.129)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (0.6.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker>=2.111.0->smjsindustry) (3.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker>=2.111.0->smjsindustry) (1.16.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->sagemaker>=2.111.0->smjsindustry) (0.19.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker>=2.111.0->smjsindustry) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->sagemaker>=2.111.0->smjsindustry) (2.8.2)\n",
            "Requirement already satisfied: dill>=0.3.6 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (0.3.6)\n",
            "Requirement already satisfied: multiprocess>=0.70.14 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (0.70.14)\n",
            "Requirement already satisfied: pox>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (0.3.2)\n",
            "Requirement already satisfied: ppft>=1.7.6.6 in /usr/local/lib/python3.10/dist-packages (from pathos->sagemaker>=2.111.0->smjsindustry) (1.7.6.6)\n",
            "Requirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.10/dist-packages (from schema->sagemaker>=2.111.0->smjsindustry) (0.6.0.post1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.30.0,>=1.29.129->boto3<2.0,>=1.26.28->sagemaker>=2.111.0->smjsindustry) (1.26.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, we will try to pull AMZN ticker 10k/10q filings from EDGAR and write the data as CSV to S3. Below is the single block of code that contains the API call.\n",
        "\n"
      ],
      "metadata": {
        "id": "npnr9zV_cGkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from smfinance import SECDataSetConfig, DataLoader\n",
        "from smjsindustry.finance import DataLoader\n",
        "from smjsindustry.finance.processor_config import EDGARDataSetConfig"
      ],
      "metadata": {
        "id": "1KjL6vPQb43w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extracted reports will be saved to an S3 bucket for us to review. This code will also be used in the Pipeline to fetch the report for the Ticker or CIK number passed to the SageMaker Pipeline. Executing the following code cell will run a processing job which will fetch the SEC reports from the EDGAR database.\n",
        "\n"
      ],
      "metadata": {
        "id": "s-n-YBtkcLbI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eQaWhMbccuwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtain SEC data using the SageMaker JumpStart Industry SDK\n"
      ],
      "metadata": {
        "id": "0DDZMUIWcvH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "dataset_config = EDGARDataSetConfig(\n",
        "    tickers_or_ciks=[\"amzn\", \"goog\", \"27904\", \"FB\"],  # list of stock tickers or CIKs\n",
        "    form_types=[\"10-K\", \"10-Q\"],  # list of SEC form types\n",
        "    filing_date_start=\"2019-01-01\",  # starting filing date\n",
        "    filing_date_end=\"2020-12-31\",  # ending filing date\n",
        "    email_as_user_agent=\"test-user@test.com\",\n",
        ")  # user agent email\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    role=sagemaker.get_execution_role(),  # loading job execution role\n",
        "    instance_count=1,  # instances number, limit varies with instance type\n",
        "    instance_type=\"ml.c5.2xlarge\",  # instance type\n",
        "    volume_size_in_gb=30,  # size in GB of the EBS volume to use\n",
        "    volume_kms_key=None,  # KMS key for the processing volume\n",
        "    output_kms_key=None,  # KMS key ID for processing job outputs\n",
        "    max_runtime_in_seconds=None,  # timeout in seconds. Default is 24 hours.\n",
        "    sagemaker_session=sagemaker.Session(),  # session object\n",
        "    tags=None,\n",
        ")  # a list of key-value pairs\n",
        "\n",
        "data_loader.load(\n",
        "    dataset_config,\n",
        "    \"s3://{}/{}\".format(\n",
        "        default_bucket, \"sample-sec-data\"\n",
        "    ),  # output s3 prefix (both bucket and folder names are required)\n",
        "    \"dataset_10k_10q.csv\",  # output file name\n",
        "    wait=True,\n",
        "    logs=True,\n",
        ")"
      ],
      "metadata": {
        "id": "TUQN3em6cC3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Output\n",
        "#The output of the data_loader processing job is a CSV file. We see the filings for different quarters.\n",
        "\n",
        "# The filing date comes within a month of the end date of the reporting period. Both these dates are collected and displayed in the dataframe. The column text contains the full text of the report, but the tables are not extracted. The values in the tables in the filings are balance-sheet and income-statement data (numeric/tabular) and are easily available elsewhere as they are reported in numeric databases. The last column of the dataframe comprises the Management Discussion & Analysis section, the column is named mdna, which is the primary forward-looking section in the filing. This is the section that has been most widely used in financial text analysis. Therefore, we will use the mdna text to derive the sentiment of the overall filing in this example."
      ],
      "metadata": {
        "id": "TbEFBsICc2y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data\n",
        "print(f\"{default_bucket}/{prefix}/\")\n",
        "s3_client.download_file(\n",
        "    default_bucket,\n",
        "    \"{}/{}\".format(f\"sample-sec-data\", f\"dataset_10k_10q.csv\"),\n",
        "    f\"./data/dataset_10k_10q.csv\",\n",
        ")"
      ],
      "metadata": {
        "id": "U5ms0ewmc7YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_frame_10k_10q = pd.read_csv(f\"./data/dataset_10k_10q.csv\")\n",
        "data_frame_10k_10q"
      ],
      "metadata": {
        "id": "QvLDUVIDdDaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up Your MLOps NLP Pipeline with SageMaker Pipelines\n",
        "\n"
      ],
      "metadata": {
        "id": "swonaIXheKpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Data pre-processing - extract SEC data and news about the company"
      ],
      "metadata": {
        "id": "psZCmcyPeUwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a processing step to prepare SEC data for inference"
      ],
      "metadata": {
        "id": "6FMyToQKeYc-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define a processing step to extract 10K and 10Q forms for a specific Organization either using the company Stock Ticker Symbol or CIK (Central Index Key) used to lookup reports in SEC's EDGAR System. You can find the company Stock Ticker Symbol to CIK Number mapping here. This step will also collect news article snippets related to the company using the NewsCatcher API."
      ],
      "metadata": {
        "id": "IPr3f2_dedTf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wPmKClG9eOum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Important:\n",
        "It is recommended to use CIKs as the input. The tickers will be internally converted to CIKs according to the mapping file.\n",
        "One ticker may map to multiple CIKs, but we only support the latest ticker to CIK mapping. Please provide the old CIKs in the input when you want historical filings. Also note that even though the Client side SDK allows you to download multiple SEC reports for multiple CIKs at a time, we will set up our data preprocessing step to grab exactly 1 SEC Report for 1 CIK (Company/Organization)."
      ],
      "metadata": {
        "id": "jrqbiM-pegmm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UDMPoYAHe36D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code snippet you provided is using the AWS SDK for Python (Boto3) to create an Amazon SageMaker ScriptProcessor object. The ScriptProcessor is used to run a script in a specified container environment for data processing tasks in Amazon SageMaker.\n",
        "\n",
        "In the code snippet, the ScriptProcessor is being created with the following parameters:\n",
        "\n",
        "command: The command to be executed in the script processor. In this case, it is set to [\"python3\"], indicating that the script processor will run a Python 3 command.\n",
        "\n",
        "image_uri: The URI of the container image to be used for the script processor. CONTAINER_IMAGE_URI is a placeholder variable that should be replaced with the actual URI of the desired container image.\n",
        "\n",
        "role: The IAM role ARN (Amazon Resource Name) that provides permissions to the script processor. The role variable should be replaced with the actual ARN of the IAM role.\n",
        "\n",
        "instance_count: The number of instances to be used for the script processor. The processing_instance_count variable should be replaced with the desired number of instances.\n",
        "\n",
        "instance_type: The EC2 instance type to be used for the script processor. The processing_instance_type variable should be replaced with the desired instance type, in this case, \"ml.c5.2xlarge\".\n",
        "\n",
        "After creating the ScriptProcessor object, it can be used for data processing tasks in Amazon SageMaker, such as running scripts on input data or generating output data.\n",
        "\n"
      ],
      "metadata": {
        "id": "awWcKpT1e4SP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loader_instance_type = \"ml.c5.2xlarge\"\n",
        "create_dataset_processor = ScriptProcessor(\n",
        "    command=[\"python3\"],\n",
        "    image_uri=CONTAINER_IMAGE_URI,\n",
        "    role=role,\n",
        "    instance_count=processing_instance_count,\n",
        "    instance_type=processing_instance_type,\n",
        ")"
      ],
      "metadata": {
        "id": "zc7m08jmehDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a processing step to process the SEC data for inference:\n",
        "\n"
      ],
      "metadata": {
        "id": "gnPkiKIUfA5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The additional code you provided is using the ScriptProcessor object to create a processing step in Amazon SageMaker. The processing step will execute a script called \"data-processing.py\" on input data and generate two processing outputs.\n",
        "\n",
        "Here's a breakdown of the additional code:\n",
        "\n",
        "create_dataset_script_uri: It represents the S3 URI where the \"data-processing.py\" script will be stored. It uses the default_bucket, prefix, and \"code/data-processing.py\" to construct the URI.\n",
        "\n",
        "s3_client.upload_file: This code uploads the local file \"./scripts/data-processing.py\" to the S3 bucket specified by default_bucket and the key specified by the combination of prefix and \"code/data-processing.py\". This step is necessary to make the script accessible for the processing step.\n",
        "\n",
        "create_dataset_step: It creates a ProcessingStep object, representing the processing step in Amazon SageMaker. The parameters for the ProcessingStep include:\n",
        "\n",
        "name: The name of the processing step, in this case, \"HFSECFinBertCreateDataset\".\n",
        "\n",
        "processor: The create_dataset_processor object, which is an instance of the previously created ScriptProcessor.\n",
        "\n",
        "outputs: A list of ProcessingOutput objects that define the outputs generated by the processing step. In this case, there are two outputs: \"report_data\" and \"article_data\". Each output specifies a source directory in the script processor container (\"/opt/ml/processing/output/10k10q\" and \"/opt/ml/processing/output/articles\") and a destination directory where the output data will be stored. The destination directory is constructed using the inference_input_data variable and a subdirectory name.\n",
        "\n",
        "job_arguments: A list of command-line arguments to be passed to the script. These arguments provide configuration information for the script, such as the ticker CIK, instance type, region, S3 bucket, prefix, and role. The values for these arguments are provided using the corresponding variables.\n",
        "\n",
        "code: The S3 URI of the script to be executed, which is set to create_dataset_script_uri. It specifies the location where the \"data-processing.py\" script is stored in S3."
      ],
      "metadata": {
        "id": "w1LbRhC0fVfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "create_dataset_script_uri = f\"s3://{default_bucket}/{prefix}/code/data-processing.py\"\n",
        "s3_client.upload_file(\n",
        "    Filename=\"./scripts/data-processing.py\",\n",
        "    Bucket=default_bucket,\n",
        "    Key=f\"{prefix}/code/data-processing.py\",\n",
        ")\n",
        "\n",
        "create_dataset_step = ProcessingStep(\n",
        "    name=\"HFSECFinBertCreateDataset\",\n",
        "    processor=create_dataset_processor,\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"report_data\",\n",
        "            source=\"/opt/ml/processing/output/10k10q\",\n",
        "            destination=f\"{inference_input_data}/10k10q\",\n",
        "        ),\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"article_data\",\n",
        "            source=\"/opt/ml/processing/output/articles\",\n",
        "            destination=f\"{inference_input_data}/articles\",\n",
        "        ),\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--ticker-cik\",\n",
        "        inference_ticker_cik,\n",
        "        \"--instance-type\",\n",
        "        loader_instance_type,\n",
        "        \"--region\",\n",
        "        region,\n",
        "        \"--bucket\",\n",
        "        default_bucket,\n",
        "        \"--prefix\",\n",
        "        prefix,\n",
        "        \"--role\",\n",
        "        role,\n",
        "    ],\n",
        "    code=create_dataset_script_uri,\n",
        ")"
      ],
      "metadata": {
        "id": "LCotcTIjfBY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Create models for summarization and sentiment analysis"
      ],
      "metadata": {
        "id": "I348YLMufwIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model_name = \"HFSECFinbertModel\"\n",
        "summarization_model_name = \"HFSECPegasusModel\""
      ],
      "metadata": {
        "id": "h0G1sMASfxiP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the finBert model for Sentiment Analysis"
      ],
      "metadata": {
        "id": "c_hYk6cNgYX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download pre-trained model using HuggingFaceModel class\n",
        "from sagemaker.huggingface import HuggingFaceModel\n",
        "\n",
        "hub = {\"HF_MODEL_ID\": \"ProsusAI/finbert\", \"HF_TASK\": \"text-classification\"}\n",
        "\n",
        "# create Hugging Face Model Class (documentation here: https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model)\n",
        "sentiment_huggingface_model = HuggingFaceModel(\n",
        "    name=sentiment_model_name,\n",
        "    transformers_version=\"4.6.1\",\n",
        "    pytorch_version=\"1.7.1\",\n",
        "    py_version=\"py36\",\n",
        "    env=hub,\n",
        "    role=role,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
        "\n",
        "create_sentiment_model_step = CreateModelStep(\n",
        "    name=\"HFSECFinBertCreateModel\",\n",
        "    model=sentiment_huggingface_model,\n",
        "    inputs=inputs,\n",
        "    #     depends_on=['HFSECFinBertCreateDataset']\n",
        ")"
      ],
      "metadata": {
        "id": "cJ7G4l2Xf01L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Pegasus summarization model"
      ],
      "metadata": {
        "id": "TMzDIgjPgf2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hub = {\n",
        "    \"HF_MODEL_ID\": \"human-centered-summarization/financial-summarization-pegasus\",\n",
        "    \"HF_TASK\": \"summarization\",\n",
        "}\n",
        "\n",
        "# create Hugging Face Model Class (documentation here: https://sagemaker.readthedocs.io/en/stable/frameworks/huggingface/sagemaker.huggingface.html#hugging-face-model)\n",
        "summary_huggingface_model = HuggingFaceModel(\n",
        "    name=summarization_model_name,\n",
        "    transformers_version=\"4.6.1\",\n",
        "    pytorch_version=\"1.7.1\",\n",
        "    py_version=\"py36\",\n",
        "    env=hub,\n",
        "    role=role,\n",
        "    sagemaker_session=sagemaker_session,\n",
        ")\n",
        "\n",
        "create_summary_model_step = CreateModelStep(\n",
        "    name=\"HFSECPegasusCreateModel\",\n",
        "    model=summary_huggingface_model,\n",
        "    inputs=inputs,\n",
        "    #     depends_on=['HFSECFinBertCreateDataset']\n",
        ")"
      ],
      "metadata": {
        "id": "cQbQ8hHcghLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Register model"
      ],
      "metadata": {
        "id": "00XAcSJmhYIJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use HuggingFace register method to register Hugging Face Model for deployment. Set up step as a custom processing step\n",
        "\n"
      ],
      "metadata": {
        "id": "CMY2pgKfhdCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model_package_group_name = \"HuggingFaceSECSentimentModelPackageGroup\"\n",
        "summary_model_package_group_name = \"HuggingFaceSECSummaryModelPackageGroup\"\n",
        "model_approval_status = \"Approved\"\n",
        "\n",
        "register_sentiment_model_step = RegisterModel(\n",
        "    name=\"HFSECFinBertRegisterModel\",\n",
        "    model=sentiment_huggingface_model,\n",
        "    content_types=[\"application/json\"],\n",
        "    response_types=[\"application/json\"],\n",
        "    inference_instances=[\"ml.t2.medium\", \"ml.m4.4xlarge\"],\n",
        "    transform_instances=[\"ml.m4.4xlarge\"],\n",
        "    model_package_group_name=sentiment_model_package_group_name,\n",
        "    approval_status=model_approval_status,\n",
        "    depends_on=[\"HFSECFinBertCreateModel\"],\n",
        ")\n",
        "\n",
        "register_summary_model_step = RegisterModel(\n",
        "    name=\"HFSECPegasusRegisterModel\",\n",
        "    model=summary_huggingface_model,\n",
        "    content_types=[\"application/json\"],\n",
        "    response_types=[\"application/json\"],\n",
        "    inference_instances=[\"ml.t2.medium\", \"ml.m4.4xlarge\"],\n",
        "    transform_instances=[\"ml.m4.4xlarge\"],\n",
        "    model_package_group_name=summary_model_package_group_name,\n",
        "    approval_status=model_approval_status,\n",
        "    depends_on=[\"HFSECPegasusCreateModel\"],\n",
        ")"
      ],
      "metadata": {
        "id": "ysQs-JpnhZvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Deploy model"
      ],
      "metadata": {
        "id": "or_SCccdh5up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We deploy the FinBert and Pegasus models from the model registry.\n",
        "\n",
        "NOTE: The models in the model registry are the pre-trained version from HuggingFace Model Hub. Each of the deployment step will attempt to deploy a SageMaker Endpoint with the model and will write a property file upon successful completion. The Pipeline will make use of these property files to decide whether to execute the subsequent summarization and sentiment analysis inference steps."
      ],
      "metadata": {
        "id": "O-UPRySwh-r0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deploy_model_instance_type = \"ml.m4.4xlarge\"\n",
        "deploy_model_instance_count = \"1\"\n",
        "\n",
        "sentiment_endpoint_name = \"HFSECFinBertModel-endpoint\"\n",
        "summarization_endpoint_name = \"HFSECPegasusModel-endpoint\""
      ],
      "metadata": {
        "id": "NVqRWg48h7Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s3_client.upload_file(\n",
        "    Filename=\"./scripts/model_deploy_v2.py\",\n",
        "    Bucket=default_bucket,\n",
        "    Key=f\"{prefix}/code/model_deploy_v2.py\",\n",
        ")\n",
        "deploy_model_script_uri = f\"s3://{default_bucket}/{prefix}/code/model_deploy_v2.py\"\n",
        "\n",
        "\n",
        "deploy_model_processor = ScriptProcessor(\n",
        "    command=[\"python3\"],\n",
        "    image_uri=CONTAINER_IMAGE_URI,\n",
        "    role=role,\n",
        "    instance_count=processing_instance_count,\n",
        "    instance_type=processing_instance_type,\n",
        ")\n",
        "\n",
        "sentiment_deploy_response = PropertyFile(\n",
        "    name=\"SentimentPropertyFile\",\n",
        "    output_name=\"sentiment_deploy_response\",\n",
        "    path=\"success.json\",  # the property file generated by the script\n",
        ")\n",
        "\n",
        "sentiment_deploy_step = ProcessingStep(\n",
        "    name=\"HFSECFinBertDeployModel\",\n",
        "    processor=deploy_model_processor,\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"sentiment_deploy_response\",\n",
        "            source=\"/opt/ml/processing/output\",\n",
        "            destination=f\"s3://{default_bucket}/{prefix}/nlp-pipeline/sentimentResponse\",\n",
        "        )\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--initial-instance-count\",\n",
        "        deploy_model_instance_count,\n",
        "        \"--endpoint-instance-type\",\n",
        "        deploy_model_instance_type,\n",
        "        \"--endpoint-name\",\n",
        "        sentiment_endpoint_name,\n",
        "        \"--model-package-group-name\",\n",
        "        sentiment_model_package_group_name,\n",
        "        \"--role\",\n",
        "        role,\n",
        "        \"--region\",\n",
        "        region,\n",
        "    ],\n",
        "    property_files=[sentiment_deploy_response],\n",
        "    code=deploy_model_script_uri,\n",
        "    depends_on=[\"HFSECFinBertRegisterModel\"],\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nz5djWa1iMQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_deploy_response = PropertyFile(\n",
        "    name=\"SummaryPropertyFile\",\n",
        "    output_name=\"summary_deploy_response\",\n",
        "    path=\"success.json\",  # the property file generated by the script\n",
        ")\n",
        "\n",
        "summary_deploy_step = ProcessingStep(\n",
        "    name=\"HFSECPegasusDeployModel\",\n",
        "    processor=deploy_model_processor,\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"summary_deploy_response\",\n",
        "            source=\"/opt/ml/processing/output\",\n",
        "            destination=f\"s3://{default_bucket}/{prefix}/nlp-pipeline/summaryResponse\",\n",
        "        )\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--initial-instance-count\",\n",
        "        deploy_model_instance_count,\n",
        "        \"--endpoint-instance-type\",\n",
        "        deploy_model_instance_type,\n",
        "        \"--endpoint-name\",\n",
        "        summarization_endpoint_name,\n",
        "        \"--model-package-group-name\",\n",
        "        summary_model_package_group_name,\n",
        "        \"--role\",\n",
        "        role,\n",
        "        \"--region\",\n",
        "        region,\n",
        "    ],\n",
        "    property_files=[summary_deploy_response],\n",
        "    code=deploy_model_script_uri,\n",
        "    depends_on=[\"HFSECPegasusRegisterModel\"],\n",
        ")"
      ],
      "metadata": {
        "id": "f-0JocpLiV3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create pipeline conditions to check if the Endpoint deployments were successful"
      ],
      "metadata": {
        "id": "unU4J8xais9u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will define a condition that checks to see if our model deployment was successful based on the property files generated by the deployment steps of both the FinBert and Pegasus Models. If both the conditions evaluates to True then we will run or subsequent inferences for Summarization and Sentiment analysis.\n",
        "\n"
      ],
      "metadata": {
        "id": "Pmyz5TOSiwQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sagemaker.workflow.conditions import ConditionEquals\n",
        "from sagemaker.workflow.condition_step import ConditionStep\n",
        "from sagemaker.workflow.functions import JsonGet\n",
        "\n",
        "summarize_script_uri = f\"s3://{default_bucket}/{prefix}/code/summarize.py\"\n",
        "\n",
        "sentiment_condition_eq = ConditionEquals(\n",
        "    left=JsonGet(  # the left value of the evaluation expression\n",
        "        step_name=\"HFSECFinBertDeployModel\",  # the step from which the property file will be grabbed\n",
        "        property_file=sentiment_deploy_response,  # the property file instance that was created earlier in Step 4\n",
        "        json_path=\"model_created\",  # the JSON path of the property within the property file success.json\n",
        "    ),\n",
        "    right=\"Y\",  # the right value of the evaluation expression, i.e. the AUC threshold\n",
        ")"
      ],
      "metadata": {
        "id": "GTUu8UtJiuJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_condition_eq = ConditionEquals(\n",
        "    left=JsonGet(  # the left value of the evaluation expression\n",
        "        step_name=\"HFSECPegasusDeployModel\",  # the step from which the property file will be grabbed\n",
        "        property_file=summary_deploy_response,  # the property file instance that was created earlier in Step 4\n",
        "        json_path=\"model_created\",  # the JSON path of the property within the property file success.json\n",
        "    ),\n",
        "    right=\"Y\",  # the right value of the evaluation expression, i.e. the AUC threshold\n",
        ")\n",
        "\n",
        "summarize_processor = ScriptProcessor(\n",
        "    command=[\"python3\"],\n",
        "    image_uri=CONTAINER_IMAGE_URI,\n",
        "    role=role,\n",
        "    instance_count=processing_instance_count,\n",
        "    instance_type=processing_instance_type,\n",
        ")\n",
        "\n",
        "summarize_step_2 = ProcessingStep(\n",
        "    name=\"HFSECPegasusSummarizer_2\",\n",
        "    processor=summarize_processor,\n",
        "    inputs=[\n",
        "        sagemaker.processing.ProcessingInput(\n",
        "            input_name=\"summary_data\",\n",
        "            source=f\"{inference_input_data}/10k10q\",\n",
        "            destination=\"/opt/ml/processing/input\",\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"summarized_data\",\n",
        "            source=\"/opt/ml/processing/output\",\n",
        "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
        "        )\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--ticker-cik\",\n",
        "        inference_ticker_cik,\n",
        "        \"--region\",\n",
        "        region,\n",
        "        \"--endpoint-name\",\n",
        "        summarization_endpoint_name,\n",
        "    ],\n",
        "    code=summarize_script_uri,\n",
        ")"
      ],
      "metadata": {
        "id": "ThM1F7OwjDyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "deploy_condition_step = ConditionStep(...) : This code block initializes a condition step using the ConditionStep class and assigns it to the deploy_condition_step variable. The condition step is used to define branching logic based on specified conditions."
      ],
      "metadata": {
        "id": "pIwMaIapuohq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters for the ConditionStep class include:\n",
        "\n",
        "name=\"HFSECFinBertDeployConditionCheck\" : Specifies the name of the condition step.\n",
        "\n",
        "conditions=[sentiment_condition_eq, summary_condition_eq] : Defines the conditions for the step. The conditions are specified using the sentiment_condition_eq and summary_condition_eq variables. These conditions likely use the ConditionEquals class or a similar condition class from the sagemaker.workflow.conditions module.\n",
        "\n",
        "if_steps=[summarize_step_2] : Specifies the steps to execute if the conditions evaluate to True. In this case, the summarize_step_2 step is included in the if_steps list. It is assumed that the summarize_step_2 step is defined elsewhere in the pipeline.\n",
        "\n",
        "else_steps=[] : Specifies the steps to execute if the conditions evaluate to False. In this case, the else_steps list is empty, indicating that there are no steps to execute in the else branch.\n",
        "\n",
        "depends_on=[\"HFSECFinBertDeployModel\", \"HFSECPegasusDeployModel\"] : Specifies the dependencies for the step. The step depends on the completion of both the \"HFSECFinBertDeployModel\" and \"HFSECPegasusDeployModel\" steps. These steps likely correspond to the deployment of the Finbert and Pegasus models in the pipeline."
      ],
      "metadata": {
        "id": "YnC_fOChusTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "deploy_condition_step = ConditionStep(\n",
        "    name=\"HFSECFinBertDeployConditionCheck\",\n",
        "    conditions=[\n",
        "        sentiment_condition_eq,\n",
        "        summary_condition_eq,\n",
        "    ],  # the equal to conditions defined above\n",
        "    if_steps=[\n",
        "        summarize_step_2\n",
        "    ],  # if the condition evaluates to true then run the summarization step\n",
        "    else_steps=[],  # there are no else steps so we will keep it empty\n",
        "    depends_on=[\n",
        "        \"HFSECFinBertDeployModel\",\n",
        "        \"HFSECPegasusDeployModel\",\n",
        "    ],  # dependencies on both Finbert and Pegasus Deployment steps\n",
        ")"
      ],
      "metadata": {
        "id": "vwCXWzXvjIpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Summarize SEC report step\n"
      ],
      "metadata": {
        "id": "Hjr6w78MjO8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step is to make use of the Pegasus Summarizer model endpoint to summarize the MDNA text from the SEC report. Because the MDNA text is usually large, we want to derive a short summary of the overall text to be able to determine the overall sentiment.\n",
        "\n"
      ],
      "metadata": {
        "id": "9u7Muc51jSW3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment_processor = ScriptProcessor(...) : This line creates an instance of the ScriptProcessor class and assigns it to the sentiment_processor variable. The ScriptProcessor class is used to run a script in a specified container environment."
      ],
      "metadata": {
        "id": "maI4cv32uWCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters for the ScriptProcessor class include:\n",
        "\n",
        "command=[\"python3\"] : Specifies the command to execute the script. In this case, the command is set to [\"python3\"], indicating that the script should be executed using Python 3.\n",
        "\n",
        "image_uri=CONTAINER_IMAGE_URI : Specifies the URI of the container image to use for running the script. The CONTAINER_IMAGE_URI variable should hold the URI of the desired container image.\n",
        "\n",
        "role=role : Specifies the IAM role to use for the script processor. The role variable should hold the ARN of the IAM role with the necessary permissions to run the script.\n",
        "\n",
        "instance_count=processing_instance_count : Specifies the number of instances to use for running the script. The processing_instance_count variable should hold the desired instance count.\n",
        "\n",
        "instance_type=processing_instance_type : Specifies the type of instance to use for running the script. The processing_instance_type variable should hold the desired instance type."
      ],
      "metadata": {
        "id": "ETfiOyAXuZj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_processor = ScriptProcessor(\n",
        "    command=[\"python3\"],\n",
        "    image_uri=CONTAINER_IMAGE_URI,\n",
        "    role=role,\n",
        "    instance_count=processing_instance_count,\n",
        "    instance_type=processing_instance_type,\n",
        ")"
      ],
      "metadata": {
        "id": "d35ENiibjPsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "s3_client.upload_file(...) : This line uploads the sentiment analysis script file to the specified S3 location."
      ],
      "metadata": {
        "id": "O4hFilB6tzw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3_client.upload_file(\n",
        "    Filename=\"./scripts/summarize.py\", Bucket=default_bucket, Key=f\"{prefix}/code/summarize.py\"\n",
        ")\n",
        "\n",
        "summarize_step_1 = ProcessingStep(\n",
        "    name=\"HFSECPegasusSummarizer_1\",\n",
        "    processor=summarize_processor,\n",
        "    inputs=[\n",
        "        sagemaker.processing.ProcessingInput(\n",
        "            input_name=\"summary_data\",\n",
        "            source=f\"{inference_input_data}/10k10q\",\n",
        "            destination=\"/opt/ml/processing/input\",\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"summarized_data\",\n",
        "            source=\"/opt/ml/processing/output\",\n",
        "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
        "        )\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--ticker-cik\",\n",
        "        inference_ticker_cik,\n",
        "        \"--region\",\n",
        "        region,\n",
        "        \"--endpoint-name\",\n",
        "        summarization_endpoint_name,\n",
        "    ],\n",
        "    code=summarize_script_uri,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "summarize_step_2 = ProcessingStep(\n",
        "    name=\"HFSECPegasusSummarizer_2\",\n",
        "    processor=summarize_processor,\n",
        "    inputs=[\n",
        "        sagemaker.processing.ProcessingInput(\n",
        "            input_name=\"summary_data\",\n",
        "            source=f\"{inference_input_data}/10k10q\",\n",
        "            destination=\"/opt/ml/processing/input\",\n",
        "        )\n",
        "    ],\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"summarized_data\",\n",
        "            source=\"/opt/ml/processing/output\",\n",
        "            destination=f\"{inference_input_data}/10k10q/summary\",\n",
        "        )\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--ticker-cik\",\n",
        "        inference_ticker_cik,\n",
        "        \"--region\",\n",
        "        region,\n",
        "        \"--endpoint-name\",\n",
        "        summarization_endpoint_name,\n",
        "    ],\n",
        "    code=summarize_script_uri,\n",
        ")"
      ],
      "metadata": {
        "id": "K8sv5LpwoMAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Sentiment inference step - SEC summary and news articles"
      ],
      "metadata": {
        "id": "AG1abZ5DoTOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step uses the MDNA summary (determined by the previous step) and the news articles to find out the sentiment of the company's financial and what the Market trends are indicating. This would help us understand the overall position of the company's financial outlook and current position without leaning solely on the company's forward-looking statements and bring objective market opinions into the picture.\n",
        "\n"
      ],
      "metadata": {
        "id": "DGqdZ2APoW9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_processor = ScriptProcessor(\n",
        "    command=[\"python3\"],\n",
        "    image_uri=CONTAINER_IMAGE_URI,\n",
        "    role=role,\n",
        "    instance_count=processing_instance_count,\n",
        "    instance_type=processing_instance_type,\n",
        ")"
      ],
      "metadata": {
        "id": "AlNT5whhoUXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_script_uri = f\"s3://{default_bucket}/{prefix}/code/sentiment.py\"\n",
        "s3_client.upload_file(\n",
        "    Filename=\"./scripts/sentiment.py\", Bucket=default_bucket, Key=f\"{prefix}/code/sentiment.py\"\n",
        ")\n",
        "\n",
        "sentiment_step_1 = ProcessingStep(\n",
        "    name=\"HFSECFinBertSentiment_1\",\n",
        "    processor=summarize_processor,\n",
        "    inputs=[\n",
        "        sagemaker.processing.ProcessingInput(\n",
        "            input_name=\"sec_summary\",\n",
        "            source=f\"{inference_input_data}/10k10q/summary\",\n",
        "            destination=\"/opt/ml/processing/input/10k10q\",\n",
        "        ),\n",
        "        sagemaker.processing.ProcessingInput(\n",
        "            input_name=\"articles\",\n",
        "            source=f\"{inference_input_data}/articles\",\n",
        "            destination=\"/opt/ml/processing/input/articles\",\n",
        "        ),\n",
        "    ],\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"sentiment_data\",\n",
        "            source=\"/opt/ml/processing/output\",\n",
        "            destination=f\"{inference_input_data}/sentiment\",\n",
        "        )\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--ticker-cik\",\n",
        "        inference_ticker_cik,\n",
        "        \"--region\",\n",
        "        region,\n",
        "        \"--endpoint-name\",\n",
        "        sentiment_endpoint_name,\n",
        "    ],\n",
        "    code=sentiment_script_uri,\n",
        "    depends_on=[\"HFSECPegasusSummarizer_1\"],\n",
        ")\n",
        "\n",
        "sentiment_step_2 = ProcessingStep(\n",
        "    name=\"HFSECFinBertSentiment_2\",\n",
        "    processor=summarize_processor,\n",
        "    inputs=[\n",
        "        sagemaker.processing.ProcessingInput(\n",
        "            input_name=\"sec_summary\",\n",
        "            source=f\"{inference_input_data}/10k10q/summary\",\n",
        "            destination=\"/opt/ml/processing/input/10k10q\",\n",
        "        ),\n",
        "        sagemaker.processing.ProcessingInput(\n",
        "            input_name=\"articles\",\n",
        "            source=f\"{inference_input_data}/articles\",\n",
        "            destination=\"/opt/ml/processing/input/articles\",\n",
        "        ),\n",
        "    ],\n",
        "    outputs=[\n",
        "        sagemaker.processing.ProcessingOutput(\n",
        "            output_name=\"sentiment_data\",\n",
        "            source=\"/opt/ml/processing/output\",\n",
        "            destination=f\"{inference_input_data}/sentiment\",\n",
        "        )\n",
        "    ],\n",
        "    job_arguments=[\n",
        "        \"--ticker-cik\",\n",
        "        inference_ticker_cik,\n",
        "        \"--region\",\n",
        "        region,\n",
        "        \"--endpoint-name\",\n",
        "        sentiment_endpoint_name,\n",
        "    ],\n",
        "    code=sentiment_script_uri,\n",
        "    depends_on=[\"HFSECPegasusSummarizer_2\"],\n",
        ")"
      ],
      "metadata": {
        "id": "XI9NKUmIob-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Condition Step\n"
      ],
      "metadata": {
        "id": "ZrNggJspoye5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explained earlier, this is a top level condition step. This step will determine based on the value of the pipeline parameter model_register_deploy on whether we want to register and deploy a new version of the models and then run inference, or to simply run inference using the existing endpoints.\n",
        "\n"
      ],
      "metadata": {
        "id": "aR3tbINGo0pS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "condition_eq = ConditionEquals(left=model_register_deploy, right=\"Y\"): This line defines a condition using the ConditionEquals class. The ConditionEquals class takes two parameters:\n",
        "\n",
        "left: Represents the left side of the equality condition. In this case, it is the model_register_deploy variable, which presumably holds a value that you want to compare.\n",
        "\n",
        "right: Represents the right side of the equality condition. In this case, it is the string \"Y\", indicating the expected value for the condition to evaluate to true."
      ],
      "metadata": {
        "id": "cEfUO5lJtRkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using the ConditionEquals class, you can define an equality condition that compares two values and returns True if they are equal. This condition can be used in a ConditionStep to control the flow of execution in an Amazon SageMaker pipeline."
      ],
      "metadata": {
        "id": "6g78KRITtVH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sagemaker.workflow.conditions import ConditionEquals\n",
        "from sagemaker.workflow.condition_step import ConditionStep\n",
        "\n",
        "condition_eq = ConditionEquals(left=model_register_deploy, right=\"Y\")"
      ],
      "metadata": {
        "id": "c8ZnXvVToy-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "condition_step = ConditionStep(...) : This line creates a new instance of the ConditionStep class and assigns it to the condition_step variable. The ConditionStep class is used to define a step in the pipeline that includes a condition for branching the execution flow."
      ],
      "metadata": {
        "id": "OVj_GYacs_cU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters for the ConditionStep class include:\n",
        "\n",
        "name=\"HFSECFinBertConditionCheck\": Specifies the name of the condition step as \"HFSECFinBertConditionCheck\". This name is used to identify the step within the pipeline.\n",
        "\n",
        "conditions=[condition_eq]: Defines the condition for the step. In this example, the condition is specified as a list containing a single condition object represented by the variable condition_eq. The condition_eq object represents an equality condition that evaluates to either true or false.\n",
        "\n",
        "if_steps=[...]: Specifies the list of steps to be executed if the condition evaluates to true. In this example, the steps are represented by the variables create_sentiment_model_step, register_sentiment_model_step, sentiment_deploy_step, create_summary_model_step, register_summary_model_step, and summary_deploy_step. These variables hold the individual steps of the pipeline that should be executed if the condition is true.\n",
        "\n",
        "else_steps=[summarize_step_1]: Specifies the list of steps to be executed if the condition evaluates to false. In this example, the summarize_step_1 variable represents a single step that should be executed if the condition is false.\n",
        "\n",
        "depends_on=[\"HFSECFinBertCreateDataset\"]: Specifies the list of steps that the condition step depends on. In this case, the condition step depends on the step named \"HFSECFinBertCreateDataset\". This ensures that the condition step is executed after the \"HFSECFinBertCreateDataset\" step.\n",
        "\n"
      ],
      "metadata": {
        "id": "rGzwPMgztDjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the condition step\n",
        "condition_step = ConditionStep(\n",
        "    name=\"HFSECFinBertConditionCheck\",\n",
        "    conditions=[condition_eq],  # the parameter is Y\n",
        "    if_steps=[\n",
        "        create_sentiment_model_step,\n",
        "        register_sentiment_model_step,\n",
        "        sentiment_deploy_step,\n",
        "        create_summary_model_step,\n",
        "        register_summary_model_step,\n",
        "        summary_deploy_step,\n",
        "    ],  # if the condition evaluates to true then create model, register, and deploy\n",
        "    else_steps=[summarize_step_1],\n",
        "    depends_on=[\"HFSECFinBertCreateDataset\"],\n",
        ")"
      ],
      "metadata": {
        "id": "cxS0L_QAo6Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combine Pipeline steps and run\n"
      ],
      "metadata": {
        "id": "8FZw2t3Ho9AE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline_name = \"FinbertSECDeploymentPipeline\": This line assigns the name \"FinbertSECDeploymentPipeline\" to the pipeline_name variable. This name will be used as the identifier for the pipeline.\n",
        "\n",
        "pipeline = Pipeline(...) : This line creates a new instance of the Pipeline class. "
      ],
      "metadata": {
        "id": "2cb6TZLcslM7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameters for the Pipeline class include:\n",
        "\n",
        "name=pipeline_name: Specifies the name of the pipeline using the pipeline_name variable.\n",
        "\n",
        "parameters=[...]: Defines a list of parameters that can be passed to the pipeline during execution. In this case, the parameters are represented by the variables processing_instance_type, processing_instance_count, model_register_deploy, inference_ticker_cik, and inference_input_data. These variables hold the values that will be passed as input parameters to the pipeline.\n",
        "\n",
        "steps=[...]: Specifies the list of steps that make up the pipeline. In this example, the steps are represented by the variables create_dataset_step, condition_step, deploy_condition_step, sentiment_step_1, and sentiment_step_2. These variables hold the individual steps of the pipeline, which can include data processing, conditions, model deployment, or other tasks."
      ],
      "metadata": {
        "id": "MmocTRmqsp_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_name = \"FinbertSECDeploymentPipeline\"\n",
        "pipeline = Pipeline(\n",
        "    name=pipeline_name,\n",
        "    parameters=[\n",
        "        processing_instance_type,\n",
        "        processing_instance_count,\n",
        "        model_register_deploy,\n",
        "        inference_ticker_cik,\n",
        "        inference_input_data,\n",
        "    ],\n",
        "    steps=[\n",
        "        create_dataset_step,\n",
        "        condition_step,\n",
        "        deploy_condition_step,\n",
        "        sentiment_step_1,\n",
        "        sentiment_step_2,\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "4jgpuMjEo8cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline.upsert(role_arn=role): This line invokes the upsert() method on the pipeline object to create or update the pipeline. The role_arn parameter is used to specify the Amazon Resource Name (ARN) of the IAM role that grants necessary permissions for the pipeline. The role variable represents the ARN of the IAM role"
      ],
      "metadata": {
        "id": "tdmYtVWasVjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "By calling upsert() with the appropriate parameters, you define or modify the pipeline in Amazon SageMaker, allowing you to manage and orchestrate your machine learning workflows."
      ],
      "metadata": {
        "id": "PevFZBdssYO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.upsert(role_arn=role)\n"
      ],
      "metadata": {
        "id": "FW5LmBvspCfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The %%time magic command provides timing information for the overall execution"
      ],
      "metadata": {
        "id": "uDt31nrJr6hQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start_response = pipeline.start(): This line initiates the execution of the pipeline. The start() method is called on the pipeline object, and the response from starting the pipeline is assigned to the start_response variable. This response contains information about the pipeline execution, such as the execution ID."
      ],
      "metadata": {
        "id": "zP1KgcpbrvKl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start_response.wait(delay=60, max_attempts=200): This line waits for the pipeline execution to complete. It uses the wait() method of the start_response object to wait for the execution status to reach a terminal state. The delay parameter specifies the number of seconds to wait between polling for the status, and the max_attempts parameter sets the maximum number of polling attempts."
      ],
      "metadata": {
        "id": "wX5LGZGHryk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "start_response.describe(): This line retrieves the description of the pipeline execution. The describe() method is called on the start_response object, and it returns information about the execution, including its status, steps, and metadata."
      ],
      "metadata": {
        "id": "8CqJXvPXr1-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "start_response = pipeline.start()\n",
        "start_response.wait(delay=60, max_attempts=200)\n",
        "start_response.describe()"
      ],
      "metadata": {
        "id": "lvD7zfd5pEn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## View Evaluation Results\n"
      ],
      "metadata": {
        "id": "8Gfe0g7vpnyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the pipeline execution completes, we can download the evaluation data from S3 and view it.\n",
        "\n"
      ],
      "metadata": {
        "id": "7fGhb_y0pzSv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "s3_client.download_file: This line downloads a file from Amazon S3 to the local file system. It uses the download_file method of the s3_client object to perform the download.\n",
        "\n",
        "default_bucket: The name of the S3 bucket where the file is located.\n",
        "\n",
        "f\"{prefix}/nlp-pipeline/inf-data/sentiment/{ticker}_sentiment_result.csv\": The key (path) of the file within the S3 bucket. It is constructed using the prefix variable, along with additional directory structure and the name of the CSV file.\n",
        "\n",
        "f\"./data/{ticker}_sentiment_result.csv\": The local file path where the downloaded file will be saved. It is constructed using the ticker variable (presumably representing a specific stock ticker) to generate the name of the CSV file."
      ],
      "metadata": {
        "id": "4QYiQpSxrXWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sentiment_df = pd.read_csv(f\"./data/{ticker}_sentiment_result.csv\"): This line reads the downloaded CSV file into a pandas DataFrame. It uses the read_csv function from the pandas library, passing the local file path as the argument.\n",
        "\n",
        "sentiment_df: This line outputs the sentiment_df DataFrame, displaying its contents in the output. This allows you to see the data stored in the DataFrame."
      ],
      "metadata": {
        "id": "zwL27Tc3rZCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3_client.download_file(\n",
        "    default_bucket,\n",
        "    f\"{prefix}/nlp-pipeline/inf-data/sentiment/{ticker}_sentiment_result.csv\",\n",
        "    f\"./data/{ticker}_sentiment_result.csv\",\n",
        ")\n",
        "sentiment_df = pd.read_csv(f\"./data/{ticker}_sentiment_result.csv\")\n",
        "sentiment_df"
      ],
      "metadata": {
        "id": "uUn4XhlXpq8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean up\n"
      ],
      "metadata": {
        "id": "sl-xp416qT3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Delete the SageMaker Pipeline and the SageMaker Endpoints created by the pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "sEipQgizqWLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "pipeline.delete(): This line deletes the pipeline that was created. The pipeline object is assumed to be an instance of the pipeline that needs to be deleted.\n",
        "\n",
        "sagemaker_boto_client.delete_endpoint(EndpointName=sentiment_endpoint_name): This line deletes the sentiment analysis endpoint. The sagemaker_boto_client is an instance of the SageMaker Boto3 client that is used to interact with the SageMaker service. sentiment_endpoint_name is a variable that holds the name of the sentiment analysis endpoint to be deleted.\n",
        "\n",
        "sagemaker_boto_client.delete_endpoint(EndpointName=summarization_endpoint_name): This line deletes the summarization endpoint. Similar to the previous line, it uses the sagemaker_boto_client to delete the endpoint with the name specified by the summarization_endpoint_name variable.\n",
        "\n",
        "The clean_up_resources() function can be called when you want to clean up or delete the resources created during the execution of your code or when you no longer need them. "
      ],
      "metadata": {
        "id": "V5H3E6wgq7v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_up_resources():\n",
        "    # Delete the pipeline\n",
        "    pipeline.delete()\n",
        "    # Delete the sentiment analysis endpoint\n",
        "    sagemaker_boto_client.delete_endpoint(EndpointName=sentiment_endpoint_name)\n",
        "    # Delete the summarization endpoint\n",
        "    sagemaker_boto_client.delete_endpoint(EndpointName=summarization_endpoint_name)"
      ],
      "metadata": {
        "id": "l2n3U-UVqTKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}